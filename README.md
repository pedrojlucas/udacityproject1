# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This project is about optimizing a machine learning pipeline using a example dataset containinig data about clients churn in banking. We will try to predict the churn tendency of the different clients using the features present in the dataset.

For this purpose we will use two different approaches, the first one by a simple classification model using a logistic regression algorithm that is tuned by hyperdrive and the second approach is using AutoML. 

## Scikit-learn Pipeline

The frontend for all the interactions with the different part of the model is a Jupyter Notebook. On the backend we have a train.py script that does all the heavy work from a machine learning point of view, this scripts preprocess all the data and builds a dataset that has all the categorical variables transformed by one-hot encoding, all the dates are transformed to numerical format, inside the script the data is also splitted in train and test sets and finally a logistic regression algorithm is trained using two model parameters that are passed as arguments and accuracy metric is obtained.

This scripts with its arguments is used by an hyperdrive optimizer that tries to find the best hyperparameters for that model. Once this best hyperparameters are found, we train a logistic regression model with those hyperparameters and we save it.

For the hyperdrive hyperparameter search we use a random sampler, this has the advantage of producing good results without taking too much computational time.

We also implemented an early stopping policy for the hyperparameters optimization. Using an early stopping policy with this kind of optimal hyperparameters search tools like hyperdrive has the advantage of getting fastest results because if the algorithm detects that the current points selected in the hyperparameters search space are not going to improve previous results it stops the iterations and try another set of points in the hyperparameters search space. If an early stopping policy is not implemented the optimizer will continue the calculations until convergence is reached for every set of points selected for the hyperparameter search, not taking into account if they are improving or not the metric of the (in this case) classification algorithm. 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The model generated after running the AutoML pipeline was a VotingEnsembled model, this model is build by a StandardScaler for preprocessing the numerical features and a XGBoost algorithm for the classification.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
